{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "subreddit_file = open('archive/50_subreddits_list.csv', mode='r', encoding='utf-8-sig')\n",
    "subreddits = {}\n",
    "for line in subreddit_file.readlines():\n",
    "  subreddit = line.rstrip(\"\\n\").lower()\n",
    "  df = pd.read_csv(f'archive/{subreddit}.csv')\n",
    "  subreddits[subreddit] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Title and Text from DataFrames\n",
    "Our classifier only works based on Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      'Dragon Ball' Creator Akira Toryiyama Has Pass...\n",
       "1        Kaguya-sama: Love Is War - Season 3 announced! \n",
       "2                         Aqua in yoga pants | Konosuba \n",
       "3                     This is not a Cigarette [Gintama] \n",
       "4         The Devil is a Part-Timer Season 2 Announced! \n",
       "                             ...                        \n",
       "991    Mob Psycho 100 Season 2 - Episode 5 discussion...\n",
       "992    Never thought in a million years I’d come acro...\n",
       "993    I seriously hate Sundays [Engaged to the Unide...\n",
       "994                \"Spy Classroom\" New Character Visual \n",
       "995    Hayasaka Ai in Spy Suit from \"Kaguya: Love is ...\n",
       "Length: 996, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_bodies = {}\n",
    "for subreddit in subreddits:\n",
    "  df = subreddits[subreddit]\n",
    "  df['title'] = df['title'].fillna('')\n",
    "  df['body'] = df['body'].fillna('')\n",
    "  subreddit_bodies[subreddit] = df['title'] + \" \" + df['body'] \n",
    "\n",
    "subreddit_bodies['anime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Links\n",
    "Our dataset includes links of the form \"\\[link text](link)\". Since we want to analayze the text content of the subreddits, we should filter out these links and just keep \"link text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I visited North Korea recently, these are some...\n",
       "1      Taken with a phone out of my hotel window in V...\n",
       "2      Taking a ride on the Bernina Express through t...\n",
       "3      Wife and I hate big social events and love tra...\n",
       "4      The exact moment I took a step too close to th...\n",
       "                             ...                        \n",
       "992    Sisteron- France. Beautiful place we had a cof...\n",
       "993    Croatia, probably the most beautiful country i...\n",
       "994    Michelangelo's David is great, but pieta is on...\n",
       "995    If you don't mind a little dust and grit and y...\n",
       "996    I’d never realized how beautiful Montenegro wa...\n",
       "Length: 997, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Regex Library\n",
    "\n",
    "no_links = {}\n",
    "for subreddit in subreddit_bodies:\n",
    "  data = subreddit_bodies[subreddit]\n",
    "  no_links[subreddit] = data.map(lambda txt: re.sub(\"\\[([^\\]]*)\\]\\(([^\\)]*)\\)\", \" \\g<1> \", txt))\n",
    "\n",
    "no_links['travel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Since reddit posts contain long sections of prose that are unique to each user's writing we felt finding attributes about the text as a whole would be difficult. Rather, we tokenize each head and body in hopes that individual words will vary between subreddits. We also turned each word to lowercase to remove issues with capitalization between different posts, since capitalization likely doesn't affect the semantic meaning of each word in a post.\n",
    "\n",
    "Additionally, we found the special unicode character ’ (as opposed to ') in several entries across datasets likely due to differences in keyboards among different languages. We replaced the former with the latter to correctly match words with apostrophes (e.g. I’d => I'd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [new, discovery, mode, turns, video, game, ass...\n",
       "1      [we, are, not, here, to, help, you, with, your...\n",
       "2      [a, 1776, excerpt, from, john, adam's, diary, ...\n",
       "3      [famous, viking, warrior, burial, revealed, to...\n",
       "4      [3, 000, year, old, underwater, castle, discov...\n",
       "                             ...                        \n",
       "987    [dna, study, has, now, provided, support, for,...\n",
       "988    [stonehenge, megalith, came, from, scotland, n...\n",
       "989    [french, resistance, man, breaks, silence, ove...\n",
       "990    [holy, grail, of, shipwrecks', to, be, raised,...\n",
       "991    [emily, wilson's, new, translation, of, the, i...\n",
       "Length: 992, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(\n",
    "  pattern=r\"[\\w']+\", # Only match words as tokens (coarsely, \\w + apostrophes)\n",
    "  gaps=False,\n",
    "  discard_empty=True # Remove empty tokens caused by markdown content\n",
    ")\n",
    "tokenized_subreddits = {}\n",
    "for subreddit in no_links:\n",
    "  data = no_links[subreddit]\n",
    "  data = data.map(lambda txt: txt.replace(\"’\", \"'\").lower())\n",
    "  tokenized_subreddits[subreddit] = data.map(lambda txt: tokenizer.tokenize(txt)) \n",
    "\n",
    "tokenized_subreddits['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my cab driver tonight was so excited to share ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guardians of the front page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gas station worker takes precautionary measure...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the conversation my son and i will have on chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the denver broncos have the entire town of sou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49261</th>\n",
       "      <td>how come no one has invented a foot pedal for ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49262</th>\n",
       "      <td>why are trans people talked about so much desp...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49263</th>\n",
       "      <td>did your penis ever fall asleep like your legs...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49264</th>\n",
       "      <td>someone stole my bike i tracked it to its loca...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49265</th>\n",
       "      <td>why are cops allowed to be fat but seriously h...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text subreddit\n",
       "0      my cab driver tonight was so excited to share ...         1\n",
       "1                            guardians of the front page         1\n",
       "2      gas station worker takes precautionary measure...         1\n",
       "3      the conversation my son and i will have on chr...         1\n",
       "4      the denver broncos have the entire town of sou...         1\n",
       "...                                                  ...       ...\n",
       "49261  how come no one has invented a foot pedal for ...        50\n",
       "49262  why are trans people talked about so much desp...        50\n",
       "49263  did your penis ever fall asleep like your legs...        50\n",
       "49264  someone stole my bike i tracked it to its loca...        50\n",
       "49265  why are cops allowed to be fat but seriously h...        50\n",
       "\n",
       "[49266 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring tokens back together as data\n",
    "subreddit_classes = {}\n",
    "subreddit_df = pd.DataFrame(columns=['text', 'subreddit'])\n",
    "\n",
    "next_class = 1\n",
    "for subreddit in tokenized_subreddits:\n",
    "  subreddit_classes[subreddit] = next_class\n",
    "  data = pd.DataFrame()\n",
    "  data['text'] = tokenized_subreddits[subreddit].map(lambda arr: ' '.join(arr))\n",
    "  data['subreddit'] = next_class\n",
    "  subreddit_df = pd.concat([subreddit_df, data], ignore_index=True)\n",
    "  next_class += 1\n",
    "\n",
    "subreddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = subreddit_df['text']\n",
    "y = subreddit_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit\n",
       "36    769\n",
       "1     769\n",
       "35    766\n",
       "3     765\n",
       "6     759\n",
       "14    759\n",
       "41    758\n",
       "39    756\n",
       "26    754\n",
       "34    754\n",
       "8     752\n",
       "43    750\n",
       "21    750\n",
       "5     750\n",
       "12    750\n",
       "44    749\n",
       "16    749\n",
       "22    748\n",
       "2     748\n",
       "25    746\n",
       "17    746\n",
       "47    744\n",
       "30    743\n",
       "48    741\n",
       "20    740\n",
       "33    739\n",
       "32    737\n",
       "19    737\n",
       "24    736\n",
       "38    736\n",
       "18    736\n",
       "15    736\n",
       "46    735\n",
       "4     735\n",
       "7     735\n",
       "9     733\n",
       "50    733\n",
       "13    733\n",
       "11    733\n",
       "49    731\n",
       "31    728\n",
       "42    727\n",
       "27    726\n",
       "37    718\n",
       "23    717\n",
       "10    711\n",
       "28    708\n",
       "29    706\n",
       "40    699\n",
       "45    669\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "reddit_classifer = Pipeline([\n",
    "  ('cv', CountVectorizer()),\n",
    "  ('tfidf', TfidfTransformer()),\n",
    "  ('sgd', SGDClassifier()),\n",
    "])\n",
    "\n",
    "reddit_classifer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6011204026954615"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = reddit_classifer.predict(X_test)\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictPrompt(prompt):\n",
    "  predicted_class = reddit_classifer.predict([prompt])[0].item()\n",
    "  for subreddit in subreddit_classes:\n",
    "    if subreddit_classes[subreddit] == predicted_class:\n",
    "      return subreddit\n",
    "  print(\"No applicable subreddit (should be unreachable)\")\n",
    "\n",
    "predictPrompt(\"My gf just broke up with me. What should I do?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
